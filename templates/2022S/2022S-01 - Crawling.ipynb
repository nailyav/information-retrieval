{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server: GitHub.com\n",
      "Date: Sun, 06 Feb 2022 23:18:30 GMT\n",
      "Content-Type: text/html; charset=utf-8\n",
      "Vary: X-PJAX, X-PJAX-Container, Accept-Language, Accept-Encoding, Accept, X-Requested-With\n",
      "permissions-policy: interest-cohort=()\n",
      "content-language: en-US\n",
      "ETag: W/\"de731bb6292e06c47193421ab93ab6e1\"\n",
      "Cache-Control: max-age=0, private, must-revalidate\n",
      "Strict-Transport-Security: max-age=31536000; includeSubdomains; preload\n",
      "X-Frame-Options: deny\n",
      "X-Content-Type-Options: nosniff\n",
      "X-XSS-Protection: 0\n",
      "Referrer-Policy: origin-when-cross-origin, strict-origin-when-cross-origin\n",
      "Expect-CT: max-age=2592000, report-uri=\"https://api.github.com/_private/browser/errors\"\n",
      "Content-Security-Policy: default-src 'none'; base-uri 'self'; block-all-mixed-content; child-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/; connect-src 'self' uploads.github.com objects-origin.githubusercontent.com www.githubstatus.com collector.githubapp.com collector.github.com api.github.com github-cloud.s3.amazonaws.com github-production-repository-file-5c1aeb.s3.amazonaws.com github-production-upload-manifest-file-7fdce7.s3.amazonaws.com github-production-user-asset-6210df.s3.amazonaws.com cdn.optimizely.com logx.optimizely.com/v1/events translator.github.com wss://alive.github.com github.githubassets.com; font-src github.githubassets.com; form-action 'self' github.com gist.github.com objects-origin.githubusercontent.com; frame-ancestors 'none'; frame-src render.githubusercontent.com viewscreen.githubusercontent.com notebooks.githubusercontent.com; img-src 'self' data: github.githubassets.com identicons.github.com collector.githubapp.com collector.github.com github-cloud.s3.amazonaws.com secured-user-images.githubusercontent.com/ *.githubusercontent.com customer-stories-feed.github.com spotlights-feed.github.com; manifest-src 'self'; media-src github.com user-images.githubusercontent.com/ github.githubassets.com; script-src github.githubassets.com; style-src 'unsafe-inline' github.githubassets.com; worker-src github.com/assets-cdn/worker/ gist.github.com/assets-cdn/worker/\n",
      "Content-Encoding: gzip\n",
      "Set-Cookie: _gh_sess=fe0Arko%2Ffae47OhlRt4zodMEDatWPmOGe9nk3tkYvZ%2FvrtVZ3RB6QJpZPsyywmc1XBrmCfMFrZgcXsyU3s7Kvr5kNBwTU6JBQ9yZy9S8oY2ubdaQBUkxBdyPhczqyrz9iMq%2B%2Fy6y6%2Br4BfpWwWNGB3X9RGD9ZiWqa8XdePdzZpuBA45Mdvs0tDLM4ioiFOJfMPaZ8rXmM9TpD6TOIA%2Bd22EI%2F%2FaPZeO%2FUCxb829e8lWrCbeJ2u7hHpiAWPCKzYFR2Bd%2BmgGVc3jaTsdmNajyfg%3D%3D--TztHhUmq0RjEu%2Bps--VCPg%2BbNfXhy3nmx00p1hrw%3D%3D; Path=/; HttpOnly; Secure; SameSite=Lax, _octo=GH1.1.679265760.1644189513; Path=/; Domain=github.com; Expires=Mon, 06 Feb 2023 23:18:33 GMT; Secure; SameSite=Lax, logged_in=no; Path=/; Domain=github.com; Expires=Mon, 06 Feb 2023 23:18:33 GMT; HttpOnly; Secure; SameSite=Lax\n",
      "Accept-Ranges: bytes\n",
      "Transfer-Encoding: chunked\n",
      "X-GitHub-Request-Id: BBC6:D910:36B1E74:393C033:62005748\n",
      "\n",
      "File saved as github.html\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def wget(url, filename):\n",
    "    # allow redirects - in case file is relocated\n",
    "    resp = requests.get(url, allow_redirects=True)\n",
    "    # this can also be 2xx, but for simplicity now we stick to 200\n",
    "    # you can also check for `resp.ok`\n",
    "    if resp.status_code != 200:\n",
    "        print(resp.status_code, resp.reason, 'for', url)\n",
    "        return\n",
    "    \n",
    "    # just to be cool and print something\n",
    "    print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "    print()\n",
    "    \n",
    "    # try to extract filename from url\n",
    "    if filename is None:\n",
    "        # start with http*, ends if ? or # appears (or none of)\n",
    "        m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "        filename = m.group(1)\n",
    "        if not filename:\n",
    "            raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "    # what will you do in case 2 websites store file with the same name?\n",
    "    if os.path.exists(filename):\n",
    "        raise OSError(f\"File {filename} already exists\")\n",
    "    \n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "        print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # parser = argparse.ArgumentParser(description='download file.')\n",
    "    # parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
    "    # parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
    "    # args = parser.parse_args()\n",
    "    # wget(args.url, args.filename)\n",
    "    url = \"https://github.com\"\n",
    "    filename = \"github.html\"\n",
    "    wget(url, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.filename = None\n",
    "        self.content = ''\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        #TODO download self.url content, store it in self.content and return True in case of success\n",
    "        page = requests.get(self.url, allow_redirects=True)\n",
    "        if page.ok:\n",
    "            self.content = page.content\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def persist(self):\n",
    "        #TODO write document content to hard drive\n",
    "        if self.filename is None:\n",
    "            # m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", self.url)\n",
    "            # self.filename = m.group(1)\n",
    "            # if not self.filename:\n",
    "            #     m = re.search(\"^http.*/([^]*)?\", self.url)\n",
    "            #     self.filename = m.group(1)\n",
    "            self.filename = self.url.replace('https://', '').replace('http://', '').replace('/', '_')\n",
    "            if not self.filename:\n",
    "                raise NameError(f\"Filename neither given, nor found for {self.url}\")\n",
    "\n",
    "        if os.path.exists(self.filename):\n",
    "            raise OSError(f\"File {self.filename} already exists\")\n",
    "            \n",
    "        with open(self.filename, 'wb') as f:\n",
    "            f.write(self.content)\n",
    "        pass\n",
    "            \n",
    "    def load(self):\n",
    "        #TODO load content from hard drive, store it in self.content and return True in case of success\n",
    "        try:\n",
    "            with open(self.filename, 'rb') as f:\n",
    "                self.content = f.read()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Parse HTML ##\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO extract plain text, images and links from the document\n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = \"\"\n",
    "\n",
    "        body = urllib.request.urlopen(self.url).read()\n",
    "        soup = BeautifulSoup(body, 'html.parser')\n",
    "\n",
    "        for link in soup.findAll('a'):\n",
    "            if link['href'] != '' and link['href'] != '#':\n",
    "                if ((link.text, link['href'])) not in self.anchors:\n",
    "                    if self.url not in link['href']:\n",
    "                        self.anchors.append((link.text, urllib.parse.urljoin(self.url, link['href'])))\n",
    "                    else:    \n",
    "                        self.anchors.append((link.text, link['href']))\n",
    "\n",
    "        images = soup.findAll('img')\n",
    "        for image in images:\n",
    "            if image['src'] not in self.images:\n",
    "                if self.url not in image['src']:\n",
    "                    self.images.append(urllib.parse.urljoin(self.url, image['src']))\n",
    "                else:\n",
    "                    self.images.append(image['src'])\n",
    "\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(self.tag_visible, texts)  \n",
    "        self.text = u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria of success**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        #TODO implement sentence parser\n",
    "        result = []\n",
    "        plain_text = self.doc.text.split(' ')\n",
    "        for word in plain_text:\n",
    "            if word != '' and word != '-':\n",
    "                 if not '.' in word:\n",
    "                    result.append(word.lower())\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        result = self.get_sentences()\n",
    "        return Counter(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 59), ('в', 30), ('иннополис', 19), ('по', 16), ('на', 14), ('университет', 12), ('области', 10), ('с', 10), ('лаборатория', 10), ('университета', 9)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        #TODO return real crawling results. Don't forget to process failures\n",
    "\n",
    "        visited: list() = list()\n",
    "        depth_link = [(source, 0)]\n",
    "\n",
    "        while len(depth_link) != 0:\n",
    "            link = depth_link.pop(0)\n",
    "            if link[1] + 1 >= depth:\n",
    "                continue\n",
    "            visited.append(link[0])\n",
    "            anchors = HtmlDocumentTextData(link[0]).doc.anchors\n",
    "            for anchor in anchors:\n",
    "                if anchor[1] in visited:\n",
    "                    continue\n",
    "                try:\n",
    "                    yield HtmlDocumentTextData(anchor[1])\n",
    "                    depth_link.append((anchor[1], link[1] + 1))\n",
    "                    visited.append(anchor[1])\n",
    "                except:\n",
    "                    print(\"Skipping\", anchor[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping https://apply.innopolis.university/en\n",
      "Skipping https://corporate.innopolis.university/en\n",
      "https://media.innopolis.university/en\n",
      "306 distinct word(s) so far\n",
      "https://innopolis.university/lk/\n",
      "680 distinct word(s) so far\n",
      "https://innopolis.university/en/about/\n",
      "1001 distinct word(s) so far\n",
      "https://innopolis.university/en/board/\n",
      "1095 distinct word(s) so far\n",
      "https://innopolis.university/en/team/\n",
      "1097 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/\n",
      "1102 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/education-academics/\n",
      "1108 distinct word(s) so far\n",
      "https://innopolis.university/en/team-structure/techcenters/\n",
      "1111 distinct word(s) so far\n",
      "Skipping https://innopolis.university/en/faculty/\n",
      "Skipping https://innopolis.university/en/faculty/\n",
      "Skipping https://career.innopolis.university/en/job/\n",
      "Skipping https://career.innopolis.university/en/\n",
      "Skipping https://innopolis.university/en/campus\n",
      "https://innopolis.university/en/contacts/\n",
      "1121 distinct word(s) so far\n",
      "Skipping https://apply.innopolis.university/en/\n",
      "Skipping https://apply.innopolis.university/en/bachelor/\n",
      "https://apply.innopolis.university/en/bachelor/CE/\n",
      "1306 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/bachelor/DS-AI/\n",
      "1375 distinct word(s) so far\n",
      "https://apply.innopolis.university/en/master/\n",
      "1435 distinct word(s) so far\n",
      "Skipping https://apply.innopolis.university/en/master/datascience/\n",
      "Skipping https://apply.innopolis.university/en/master/securityandnetworkengineering/\n",
      "Skipping https://apply.innopolis.university/en/master/development/\n",
      "Skipping https://apply.innopolis.university/en/master/robotics/\n",
      "Skipping https://apply.innopolis.university/en/master/technological-entrepreneurship/\n",
      "Skipping https://apply.innopolis.university/en/postgraduate-study/\n",
      "https://apply.innopolis.university/en/stud-life/\n",
      "1597 distinct word(s) so far\n",
      "Skipping https://innopolis.university/en/international-relations-office/\n",
      "https://innopolis.university/en/incomingstudents/\n",
      "1731 distinct word(s) so far\n",
      "Skipping https://innopolis.university/en/outgoingstudents/\n",
      "Skipping https://innopolis.university/en/teachingexcellencecenter/\n",
      "https://innopolis.university/en/writinghubhome/\n",
      "1760 distinct word(s) so far\n",
      "Skipping https://alumni.innopolis.university/\n",
      "https://innopolis.university/en/research/\n",
      "1839 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-operating-systems/\n",
      "1900 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-software-service-engineering/\n",
      "2044 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-industrializing-software/\n",
      "2113 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-bioinformatics/\n",
      "2184 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-game-development/\n",
      "2265 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-oil-gas/\n",
      "2347 distinct word(s) so far\n",
      "https://innopolis.university/en/mlkr/\n",
      "2549 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-cyberphysical-systems/\n",
      "2608 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-networks-blockchain/\n",
      "2690 distinct word(s) so far\n",
      "https://innopolis.university/en/lab-robotics/\n",
      "2837 distinct word(s) so far\n",
      "https://innopolis.university/en/proekty/activity/\n",
      "2904 distinct word(s) so far\n",
      "https://innopolis.university/en/proekty/podderzhka-innovacionnoj-deyatelnosti/\n",
      "2973 distinct word(s) so far\n",
      "https://innopolis.university/en/startupstudio/\n",
      "3027 distinct word(s) so far\n",
      "Skipping https://innopolis.university/en/internationalpartners/\n",
      "Skipping https://innopolis.university/en/organizatsiya-i-provedenie-meropriyatiy/\n",
      "https://innopolis.university/en/?special=Y\n",
      "3048 distinct word(s) so far\n",
      "https://innopolis.university/search/\n",
      "3051 distinct word(s) so far\n",
      "https://innopolis.university/\n",
      "3259 distinct word(s) so far\n",
      "Skipping https://apply.innopolis.university/en/\n",
      "https://innopolis.university/en/ido/\n",
      "3301 distinct word(s) so far\n",
      "Skipping https://dovuz.innopolis.university/\n",
      "Skipping https://career.innopolis.university/en/\n",
      "https://university.innopolis.ru/en/about/\n",
      "3301 distinct word(s) so far\n",
      "Skipping https://apply.innopolis.university/en/bachelor/\n",
      "Skipping https://apply.innopolis.university/en/postgraduate-study/\n",
      "http://www.campuslife.innopolis.ru\n",
      "3442 distinct word(s) so far\n",
      "Skipping https://innopolis.university/en/international-relations-office/\n",
      "https://media.innopolis.university/news/clobal-ai-challenge/\n",
      "3607 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-interstudents-eng/\n",
      "3641 distinct word(s) so far\n",
      "https://media.innopolis.university/news/devops-summer-school/\n",
      "3785 distinct word(s) so far\n",
      "https://media.innopolis.university/news/webinar-for-international-candidates-/\n",
      "3795 distinct word(s) so far\n",
      "https://media.innopolis.university/news/registration-innopolis-open-2020/\n",
      "3898 distinct word(s) so far\n",
      "https://media.innopolis.university/news/cyber-resilience-petrenko/\n",
      "4079 distinct word(s) so far\n",
      "https://media.innopolis.university/news/innopolis-university-extends-international-application-deadline-/\n",
      "4114 distinct word(s) so far\n",
      "https://media.innopolis.university/en/\n",
      "4114 distinct word(s) so far\n",
      "https://www.facebook.com/InnopolisU\n",
      "4114 distinct word(s) so far\n",
      "Skipping https://vk.com/innopolisu\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "4129 distinct word(s) so far\n",
      "https://www.instagram.com/innopolisu/\n",
      "4129 distinct word(s) so far\n",
      "Skipping https://apply.innopolis.ru/en/index.php\n",
      "Skipping https://university.innopolis.ru/en/cooperation/\n",
      "Skipping https://career.innopolis.university/en/\n",
      "Skipping https://panoroo.com/virtual-tours/NvQZM6B2\n",
      "https://media.innopolis.university/en/news/\n",
      "4129 distinct word(s) so far\n",
      "https://media.innopolis.university/en/events/\n",
      "4133 distinct word(s) so far\n",
      "http://www.minsvyaz.ru/en/\n",
      "4223 distinct word(s) so far\n",
      "Skipping http://минобрнауки.рф/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Done\n",
      "[('and', 1445), ('of', 1253), ('the', 883), ('in', 572), ('to', 388), ('for', 339), ('university', 320), ('lab', 303), ('research', 286), ('и', 282), ('development', 251), ('science', 241), ('software', 224), ('innopolis', 220), ('data', 207), ('education', 191), ('it', 181), ('robotics', 178), ('artificial', 175), ('a', 164)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
